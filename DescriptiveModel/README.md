# Descriptive Model

This approach verify and refine text descriptions associated with images using advanced models.
Both methods utilize CLIP's capabilities to match images with textual descriptions, enhancing the accuracy of automatically generated text in relation to visual content.


## -- ClipDescription.ipynb --

Here we're mainly using BLIP (Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation).
https://ahmed-sabir.medium.com/paper-summary-blip-bootstrapping-language-image-pre-training-for-unified-vision-language-c1df6f6c9166
https://huggingface.co/docs/transformers/model_doc/blip#

We have :
- BLIP generated description from an url
- --------------------------------- local image
- CLIP verifying the description generated by BLIP
- The use of BLIP on a larger sample to get a occurence dictionnary

The one method using CLIP, a model developed by OpenAI, evaluate how well a generated caption (BLIP) matches an image.
Loads the CLIP model and its preprocessing function.
The image is preprocessed to be suitable for CLIP (using the CLIP preprocessing function).
Both the image and the generated caption are encoded into feature vectors using CLIPâ€™s image and text encoders.
Computes the cosine similarity between the image features and the text features to determine how closely the caption matches the image.
Outputs the generated caption and its similarity score with the image, indicating the relevance of the caption to the image content.

## -- 5wordsDescription.ipynb --

Training a ResNet model on a dataset of labeled images and using CLIP to verify generated keywords against images.

Defines a custom dataset class for loading images and their associated labels from a CSV file, and applies transformations to resize and convert images into tensors.

A ResNet-50 model is loaded and modified to output probabilities for the number of unique labels in the dataset.
The model is trained using a binary cross-entropy loss function with logits and Adam optimizer over multiple epochs.
Training involves forward passes, loss calculation, and backpropagation to update model weights.
The model's state is saved as a .pth file.
Loads and sets the model to evaluation mode.
The CLIP model is also loaded to use for verifying predictions.
Takes an image, uses the trained model to predict top 10 keywords associated with the image, and ranks them.
It then uses CLIP to encode the image and predicted keywords into vectors.
The similarities between the image features and keyword features are calculated, and the top 5 matching keywords are refined and returned.