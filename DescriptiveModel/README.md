# Descriptive Model

This approach verify and refine text descriptions associated with images using advanced models.
Both methods utilize CLIP's capabilities to match images with textual descriptions, enhancing the accuracy of automatically generated text in relation to visual content.

Here we're using BLIP (Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation).
https://ahmed-sabir.medium.com/paper-summary-blip-bootstrapping-language-image-pre-training-for-unified-vision-language-c1df6f6c9166
https://huggingface.co/docs/transformers/model_doc/blip#

We have :
- BLIP generated description from an url
- --------------------------------- local image
- CLIP verifying the description generated by BLIP
- The use of BLIP on a larger sample to get a occurence dictionnary

We use a ResNet-50 model is loaded and modified to output probabilities for the number of unique labels in the dataset.
https://medium.com/@nitishkundu1993/exploring-resnet50-an-in-depth-look-at-the-model-architecture-and-code-implementation-d8d8fa67e46f
https://huggingface.co/microsoft/resnet-50