# Descriptive Model

This approach verify and refine text descriptions associated with images using advanced models.
Both methods utilize CLIP's capabilities to match images with textual descriptions, enhancing the accuracy of automatically generated text in relation to visual content.

Here we're using BLIP (Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation).
https://ahmed-sabir.medium.com/paper-summary-blip-bootstrapping-language-image-pre-training-for-unified-vision-language-c1df6f6c9166
https://huggingface.co/docs/transformers/model_doc/blip#

We have :
- BLIP generated description from an url
- --------------------------------- local image
- CLIP verifying the description generated by BLIP
- The use of BLIP on a larger sample to get a occurence dictionnary

We use a ResNet-50 model is loaded and modified to output probabilities for the number of unique labels in the dataset.
https://medium.com/@nitishkundu1993/exploring-resnet50-an-in-depth-look-at-the-model-architecture-and-code-implementation-d8d8fa67e46f
https://huggingface.co/microsoft/resnet-50

## ‚ö†Ô∏è Important Note

This approach is designed to provide comprehensive image descriptions and detect a wide variety of objects (e.g., trees, animals, everyday items). It excels at offering detailed and general descriptions of image content beyond specific labels or categories, making it a versatile tool for broad image analysis.

However, this approach may not be ideal for identifying specialized content, such as generative algorithms, which require a more tailored detection model. For tasks focused on detecting generative algorithms, a specialized model trained explicitly on algorithm-related data, such as the ResNet model with CLIP validation, is recommended. These models are optimized to recognize algorithmic patterns and outputs, providing more accurate results for such specific use cases.

## üèÉ Execution Instructions

Follow these instructions to execute the model:

### 1. Execution for `5wordsDescription.ipynb`
- This file functions similarly to the approach used in Algorithm Detection:
  - Run the first cell to train and save the ResNet-50 model. This model is modified to output probabilities for the number of unique labels in the dataset.
  - Subsequent cells can be run to provide an image and receive the top 5 keywords related to the image.
  - The model identifies the algorithm used based on the predicted keywords and displays the result.

### 2. Execution for BLIP and BLIP + CLIP Notebooks
- **Independent Cell Execution:**
  - For the BLIP and BLIP + CLIP code sections, each cell can be executed independently.
  
- **BLIP on a Larger Sample:**
  - Some cells are dedicated to running BLIP on a larger sample of images. This generates a dictionary of occurrences for each description, allowing for analysis of the frequency and accuracy of descriptions across multiple images.

### Additional Note:
- **Environment Setup:** Ensure that all dependencies, including BLIP, CLIP, and ResNet-50, are properly installed and configured. Check the links provided in the root README for installation and setup instructions for each model.

For tasks specifically focused on detecting generative algorithms, refer to the [Algorithm Detection](../AlgorithmDetection/README.md) approach for a more targeted model optimized for algorithmic pattern recognition.


