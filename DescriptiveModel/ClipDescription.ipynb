{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The one method using CLIP, a model developed by OpenAI, evaluate how well a generated caption (BLIP) matches an image.\n",
    "Loads the CLIP model and its preprocessing function.\n",
    "The image is preprocessed to be suitable for CLIP (using the CLIP preprocessing function).\n",
    "Both the image and the generated caption are encoded into feature vectors using CLIPâ€™s image and text encoders.\n",
    "Computes the cosine similarity between the image features and the text features to determine how closely the caption matches the image.\n",
    "Outputs the generated caption and its similarity score with the image, indicating the relevance of the caption to the image content.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyriancr/.pyenv/versions/3.10.8/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a black background with a white and blue pattern\n"
     ]
    }
   ],
   "source": [
    "# Using BLIP to generate a caption of the image\n",
    "# Local image\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load the model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load your image\n",
    "image_path = \"dataset/train/image29.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Preprocess the image\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the caption\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a colorful background with a grid pattern\n"
     ]
    }
   ],
   "source": [
    "# url image\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load the model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load an image from a URL\n",
    "image_url = \"https://gateway.fxhash.xyz/ipfs/Qmcg7DaHFPcgRyZ3pbwuTgNAS9CP1U7zp5HkjFfKJ9FYWZ\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "# Preprocess the image\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the caption\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: a black background with a white and blue pattern\n",
      "Similarity with image: 29.5569\n"
     ]
    }
   ],
   "source": [
    "# Using CLIP to check the BLIP generated caption\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "# Load CLIP model and preprocess\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "# Preprocess the image for CLIP\n",
    "clip_image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Encode image and generated description\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.encode_image(clip_image_input)\n",
    "    text_features = clip_model.encode_text(clip.tokenize([caption]).to(device))\n",
    "\n",
    "# Compute similarity\n",
    "similarity = (image_features @ text_features.T).item()\n",
    "\n",
    "print(f\"Caption: {caption}\")\n",
    "print(f\"Similarity with image: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption Occurrences: {'red tulip': 1, 'a black and': 6, 'an illustration of': 1, 'an image of': 10, 'a colorful abstract': 1, 'a red and': 2, 'a black background': 1, 'green abstract background': 1, 'a cartoon character': 1, 'a brown and': 1, 'a drawing of': 4, 'a group of': 1, 'black and white': 1, 'three black and': 1, 'a bunch of': 1, 'a computer generated': 2, 'blue wood texture': 1, 'the cover of': 1, 'a blue background': 1, 'a circle with': 1, 'a green and': 2, 'a painting of': 1, 'a chart showing': 1, 'this is a': 1, 'a picture of': 1, 'a colorful pattern': 1, 'a set of': 1, 'a poster with': 1, 'the logo for': 1, 'an abstract pattern': 1}\n"
     ]
    }
   ],
   "source": [
    "# Trying to use BLIP on a larger sample\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load the model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "caption_occurrences = {}\n",
    "\n",
    "# Iterate over all images\n",
    "for i in range(1, 51):\n",
    "    image_path = f\"dataset/ipfs/image{i}.png\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    prompt = \"Describe this in one word:\"\n",
    "\n",
    "    # Preprocess the image\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate the caption\n",
    "    outputs = model.generate(**inputs, max_length=4, num_beams=5)  # max_length=3 to encourage short output\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Update the caption occurrences dictionary\n",
    "    if caption in caption_occurrences:\n",
    "        caption_occurrences[caption] += 1\n",
    "    else:\n",
    "        caption_occurrences[caption] = 1\n",
    "\n",
    "# Print the final caption occurrences\n",
    "print(\"Caption Occurrences:\", caption_occurrences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an image of: 10\n",
      "a black and: 6\n",
      "a drawing of: 4\n",
      "a red and: 2\n",
      "a computer generated: 2\n",
      "a green and: 2\n",
      "red tulip: 1\n",
      "an illustration of: 1\n",
      "a colorful abstract: 1\n",
      "a black background: 1\n",
      "green abstract background: 1\n",
      "a cartoon character: 1\n",
      "a brown and: 1\n",
      "a group of: 1\n",
      "black and white: 1\n",
      "three black and: 1\n",
      "a bunch of: 1\n",
      "blue wood texture: 1\n",
      "the cover of: 1\n",
      "a blue background: 1\n",
      "a circle with: 1\n",
      "a painting of: 1\n",
      "a chart showing: 1\n",
      "this is a: 1\n",
      "a picture of: 1\n",
      "a colorful pattern: 1\n",
      "a set of: 1\n",
      "a poster with: 1\n",
      "the logo for: 1\n",
      "an abstract pattern: 1\n"
     ]
    }
   ],
   "source": [
    "sorted_captions = dict(sorted(caption_occurrences.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print the sorted dictionary\n",
    "for caption, count in sorted_captions.items():\n",
    "    print(f\"{caption}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
