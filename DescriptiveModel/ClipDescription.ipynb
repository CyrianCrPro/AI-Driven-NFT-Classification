{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyriancr/.pyenv/versions/3.10.8/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a black background with a white and blue pattern\n"
     ]
    }
   ],
   "source": [
    "# Using BLIP to generate a caption of the image\n",
    "# Local image\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load the model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load your image\n",
    "image_path = \"dataset/train/image29.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Preprocess the image\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the caption\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a colorful background with a grid pattern\n"
     ]
    }
   ],
   "source": [
    "# url image\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load the model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load an image from a URL\n",
    "image_url = \"https://gateway.fxhash.xyz/ipfs/Qmcg7DaHFPcgRyZ3pbwuTgNAS9CP1U7zp5HkjFfKJ9FYWZ\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "# Preprocess the image\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the caption\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: a black background with a white and blue pattern\n",
      "Similarity with image: 29.5569\n"
     ]
    }
   ],
   "source": [
    "# Using CLIP to check the BLIP generated caption\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "# Load CLIP model and preprocess\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "# Preprocess the image for CLIP\n",
    "clip_image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Encode image and generated description\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.encode_image(clip_image_input)\n",
    "    text_features = clip_model.encode_text(clip.tokenize([caption]).to(device))\n",
    "\n",
    "# Compute similarity\n",
    "similarity = (image_features @ text_features.T).item()\n",
    "\n",
    "print(f\"Caption: {caption}\")\n",
    "print(f\"Similarity with image: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption Occurrences: {'red tulip': 1, 'a black and': 6, 'an illustration of': 1, 'an image of': 10, 'a colorful abstract': 1, 'a red and': 2, 'a black background': 1, 'green abstract background': 1, 'a cartoon character': 1, 'a brown and': 1, 'a drawing of': 4, 'a group of': 1, 'black and white': 1, 'three black and': 1, 'a bunch of': 1, 'a computer generated': 2, 'blue wood texture': 1, 'the cover of': 1, 'a blue background': 1, 'a circle with': 1, 'a green and': 2, 'a painting of': 1, 'a chart showing': 1, 'this is a': 1, 'a picture of': 1, 'a colorful pattern': 1, 'a set of': 1, 'a poster with': 1, 'the logo for': 1, 'an abstract pattern': 1}\n"
     ]
    }
   ],
   "source": [
    "# Trying to use BLIP on a larger sample\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load the model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "caption_occurrences = {}\n",
    "\n",
    "# Iterate over all images\n",
    "for i in range(1, 51):\n",
    "    image_path = f\"dataset/ipfs/image{i}.png\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    prompt = \"Describe this in one word:\"\n",
    "\n",
    "    # Preprocess the image\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate the caption\n",
    "    outputs = model.generate(**inputs, max_length=4, num_beams=5)  # max_length=3 to encourage short output\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Update the caption occurrences dictionary\n",
    "    if caption in caption_occurrences:\n",
    "        caption_occurrences[caption] += 1\n",
    "    else:\n",
    "        caption_occurrences[caption] = 1\n",
    "\n",
    "# Print the final caption occurrences\n",
    "print(\"Caption Occurrences:\", caption_occurrences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an image of: 10\n",
      "a black and: 6\n",
      "a drawing of: 4\n",
      "a red and: 2\n",
      "a computer generated: 2\n",
      "a green and: 2\n",
      "red tulip: 1\n",
      "an illustration of: 1\n",
      "a colorful abstract: 1\n",
      "a black background: 1\n",
      "green abstract background: 1\n",
      "a cartoon character: 1\n",
      "a brown and: 1\n",
      "a group of: 1\n",
      "black and white: 1\n",
      "three black and: 1\n",
      "a bunch of: 1\n",
      "blue wood texture: 1\n",
      "the cover of: 1\n",
      "a blue background: 1\n",
      "a circle with: 1\n",
      "a painting of: 1\n",
      "a chart showing: 1\n",
      "this is a: 1\n",
      "a picture of: 1\n",
      "a colorful pattern: 1\n",
      "a set of: 1\n",
      "a poster with: 1\n",
      "the logo for: 1\n",
      "an abstract pattern: 1\n"
     ]
    }
   ],
   "source": [
    "sorted_captions = dict(sorted(caption_occurrences.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print the sorted dictionary\n",
    "for caption, count in sorted_captions.items():\n",
    "    print(f\"{caption}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
