{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyriancr/.pyenv/versions/3.10.8/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/cyriancr/.pyenv/versions/3.10.8/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7427393198013306\n",
      "Epoch 2, Loss: 0.36886802315711975\n",
      "Epoch 3, Loss: 0.18792179226875305\n",
      "Epoch 4, Loss: 0.09805647283792496\n",
      "Epoch 5, Loss: 0.04915500804781914\n",
      "Epoch 6, Loss: 0.030429232865571976\n",
      "Epoch 7, Loss: 0.02050023339688778\n",
      "Epoch 8, Loss: 0.0149346012622118\n",
      "Epoch 9, Loss: 0.011400505900382996\n",
      "Epoch 10, Loss: 0.008951690047979355\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# Custom dataset class\n",
    "class ArtDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.mlb.fit(self.labels['labels'].apply(lambda x: x.split(', ')))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.labels.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        labels = self.labels.iloc[idx, 1].split(', ')\n",
    "        labels = self.mlb.transform([labels])[0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = ArtDataset(csv_file='dataset/labels.csv', root_dir='dataset/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model\n",
    "model = resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(train_dataset.mlb.classes_))\n",
    "\n",
    "# Training\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'art_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Keywords: ['mandelbrot', 'fractal', 'particle', 'voronoi', 'pattern']\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the fine-tuned saved model\n",
    "model.load_state_dict(torch.load('art_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Load CLIP model and preprocess\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "# Define the same transform used for training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Function to predict and verify descriptions\n",
    "def predict_and_verify(image_path, model, clip_model, clip_processor, mlb):\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure the image is in RGB format\n",
    "    \n",
    "    # Preprocess the image for the classification model\n",
    "    input_image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict keywords\n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)\n",
    "        probs = torch.sigmoid(output).cpu().numpy()[0]\n",
    "        top_indices = probs.argsort()[-10:][::-1]  # Top 10 keywords\n",
    "        keywords = [mlb.classes_[idx] for idx in top_indices]\n",
    "    \n",
    "    # Preprocess the image for CLIP\n",
    "    clip_image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Encode image and generated keywords with CLIP\n",
    "    text_inputs = torch.cat([clip.tokenize(keyword) for keyword in keywords]).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(clip_image_input)\n",
    "        text_features = clip_model.encode_text(text_inputs)\n",
    "    \n",
    "    # Compute similarities\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarities = (image_features @ text_features.T).softmax(dim=-1).cpu().numpy()[0]\n",
    "    \n",
    "    # Refine and return top keywords\n",
    "    refined_keywords = sorted(zip(keywords, similarities), key=lambda x: x[1], reverse=True)[:5]\n",
    "    return [keyword for keyword, sim in refined_keywords]\n",
    "\n",
    "# Example usage\n",
    "image_path = \"image32.png\"\n",
    "keywords = predict_and_verify(image_path, model, clip_model, clip_preprocess, train_dataset.mlb)\n",
    "\n",
    "print(\"Predicted Keywords:\", keywords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
